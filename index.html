<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>My Portfolio</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
<header>
    <h2>
        <strong>TOMS BULS</strong>
        <span style="font-weight: normal;">| Data Science & Analytics Professional</span>
        <span style="font-weight: normal;">|</span>
        <a href="https://www.linkedin.com/in/tomsbuls/" target="_blank" style="text-decoration: none;">
            <img src="https://upload.wikimedia.org/wikipedia/commons/c/ca/LinkedIn_logo_initials.png"
                 alt="LinkedIn"
                 style="width: 18px; height: 18px; vertical-align: middle;">
        </a>
    </h2>
    <p>
        With proficiency in SQL, Python, R, and Power BI, I specialize in statistical analysis, machine learning, and creating intuitive visualizations to uncover actionable insights. My background in computer science and economics enhances my ability to solve complex, data-driven challenges.
    </p>
</header>

<main>
    <section class="projects">
        <div class="project full-width gpt2-project">
            <div class="content">
                <div class="text">
                    <h3>Implementing GPT-2 From Scratch</h3>
                    <div class="horizontal"></div>
                    <div class="description">
                        <p>
                            In this project, I built a <strong>GPT-2 equivalent model</strong> from scratch, defining the decoder-only Transformer architecture manually, including multi-head self-attention, feedforward layers, layer normalization, and residual connections. The model was trained on <strong>OpenWebText</strong>, a large-scale dataset with over <strong>8 million documents</strong> (~40GB, ~100M samples) for next-token prediction. To handle its size efficiently, I used dataset streaming, enabling real-time processing without loading everything into memory. The model was trained for <strong>100,000 steps</strong> to achieve high-quality text generation.
                        </p>
                        <p>
                            The model features <strong>12</strong> Transformer layers with <strong>12</strong> attention heads, a <strong>1024</strong>-dimensional feedforward network, and a dropout of 0.1 for regularization. It processes <strong>sequences of 128 tokens</strong> using <strong>768-dimensional embeddings</strong> and learnable positional encodings, with a vocabulary of <strong>50,257 tokens</strong>—matching GPT-2’s tokenizer. With <strong>124M parameters</strong>, the same as GPT-2, it balances efficiency and performance, using a shorter context length (<strong>128</strong> vs. 1024) and optimized hyperparameters. For text generation, I applied temperature scaling (1.0) and top-k sampling (k=10).
                        </p>
                        <p>
                            I trained the model on a single <strong>NVIDIA A100 GPU</strong>, using a batch size of 48 with gradient accumulation to manage memory constraints. The optimizer is AdamW with a learning rate of 3e-4, and I implemented gradient clipping with a max norm of 1.0 to prevent instability. The learning rate follows a warmup schedule for the first 2000 steps, then transitions to cosine decay for smoother convergence. To ensure training progress is preserved, I added model checkpointing every 1000 steps, allowing resumption in case of interruptions. These optimizations maximized efficiency, enabling the model to generate high-quality, structured text with remarkable fluency.
                        </p>
                    </div>
                    <div class="horizontal"></div>
                    <div class="links">
                        <span class="disabled-link">Notebook 1</span>
                        <span class="disabled-link">Notebook 2</span>
                    </div>
                </div>
                    <div class="gpt2-image">
                        <div class="gpt2-output-top">
                            <p style="font-style: italic; text-align: center; color: #777;">(Output visualization will be added here)</p>
                        </div>
                        <div class="gpt2-divider"></div> <!-- Horizontal line -->
                        <div class="gpt2-output-bottom">
                            <p style="font-style: italic; text-align: center; color: #777;">(Output visualization will be added here)</p>
                        </div>
                    </div>
            </div>
        </div>

        <div class="project full-width">
            <div class="content">
                <div class="text">
                    <h3>Transformers Architecture for Tweet Classification: Analyzing Public Sentiment in Latvia Post-2022 Elections</h3>
                    <div class="horizontal"></div>
                    <div class="description">
                        <p>
                            This project developed a semantic sentiment analysis model using the BERT architecture to classify public sentiment. A dataset of over 10,000 tweets, curated following the Autumn 2022 Latvian government elections, was manually annotated for supervised learning. The model’s performance was compared against baseline classifiers like Naive Bayes (NB) and Support Vector Machines (SVM).
                        </p>
                    </div>
                    <div class="horizontal"></div>
                    <div class="links">
                        <a href="projects/BERT_sentiment/Preprocessing.html">Data Preprocessing</a>
                        <a href="projects/BERT_sentiment/Baseline_Classifiers.html">Baseline Classifiers</a>
                        <a href="projects/BERT_sentiment/BERT_model.html">BERT Model</a>
                    </div>
                </div>
                <div class="image">
                    <a href="projects/BERT_sentiment/Baseline_Classifiers.html">
                        <img src="projects/BERT_sentiment/sentiment_tweets.png" alt="Sentiment Tweets Chart">
                    </a>
                </div>
            </div>
        </div>

        <div class="project half-width">
            <div class="content">
                <div class="text">
                    <h3>Covid-19 Analysis</h3>
                    <div class="horizontal"></div>
                    <div class="description">
                        <p>
                            This project analyzes daily COVID-19 incidence and vaccination data to uncover trends and patterns. Using deep learning models—RNN, LSTM, BiLSTM, and GRU—it forecasts future incidence and compares model performance, providing insights into the pandemic's progression.
                        </p>
                    </div>
                    <div class="horizontal"></div>
                    <div class="links">
                        <a href="projects/covid/covid_final.html">Covid Latvia Analysis Notebook</a>
                    </div>
                </div>
                <div class="image">
                    <a href="projects/covid/covid_final.html">
                        <img src="projects/covid/covid_chart.png" alt="Covid Analysis Chart">
                    </a>
                </div>
            </div>
        </div>

        <div class="project half-width">
            <div class="content">
                <div class="text">
                    <h3>E-Commerce Analysis</h3>
                    <div class="horizontal"></div>
                    <div class="description">
                        <p>
                            This notebook analyzes data from Olist, Brazil's largest e-commerce platform, using various datasets about customers, orders, products, sellers, and their geolocations. It examines delivery efficiency and customer satisfaction, leveraging location data to uncover regional trends and performance insights.
                        </p>
                    </div>
                    <div class="horizontal"></div>
                    <div class="links">
                        <a href="projects/e-commerce/e_commerce.html">E-Commerce Notebook</a>
                    </div>
                </div>
                <div class="image">
                    <a href="projects/e-commerce/e_commerce.html">
                        <img src="projects/e-commerce/e-commerce_chart.png" alt="E-Commerce Chart">
                    </a>
                </div>
            </div>
        </div>
    </section>

    <section class="power-bi-section">
        <h2>Power BI Dashboards</h2>
        <div class="power-bi-dashboard">
            <h3>Short-Term Labor Market Forecasts</h3>
            <p>TThis Power BI report presents insights from short-term labor market forecasts, detailing projected vacancy trends across regions and economic sectors. 
            The forecasts are based on detailed regional-level data on vacancies and highlight two key components: replacement demand, driven by workforce turnover, 
            and growth-driven demand, resulting from sectoral expansion.</p>
            <iframe title="Labor Market Forecasts" width="1024" height="612"
                    src="https://app.powerbi.com/view?r=eyJrIjoiOGY5ZDlmYjItMjkwOC00ZGVjLWFlYjItMTQ2MDEzNGIwYTcyIiwidCI6ImQ3NzZmZWE1LTdmOGYtNDQ0ZC1hYjRlLTVjZmZjMDg5OTVmMSIsImMiOjl9"
                    frameborder="0" allowFullScreen="true"></iframe>
        </div>

        <div class="power-bi-dashboard">
            <h3>Budget Revenues and Expenditures</h3>
            <p>This Power BI report visualizes a decade (2010–2020) of detailed State Treasury budget data, encompassing five key budgets: the central government budget, 
            the state special budget, the local government general budget, the local government special budget, and municipal donations and grants. 
            Over 600 monthly files were extracted and processed using a custom script, handling granular hierarchical classifications of revenues and expenditures across 
            5–6 levels. The report enables interactive analysis of budgetary trends and insights.</p>
            <iframe title="Budget Analysis" width="1024" height="612"
                    src="https://app.powerbi.com/view?r=eyJrIjoiMGIxMDkyODAtZWU2MS00OTU2LTk1NGEtMDRlZmE5MmYyNDhkIiwidCI6ImQ3NzZmZWE1LTdmOGYtNDQ0ZC1hYjRlLTVjZmZjMDg5OTVmMSIsImMiOjl9"
                    frameborder="0" allowFullScreen="true"></iframe>
        </div>
    </section>
</main>

<footer>
    <p>&copy; 2025 Toms Buls</p>
</footer>
</body>
</html>
